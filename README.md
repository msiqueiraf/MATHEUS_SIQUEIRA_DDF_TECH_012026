# Case T√©cnico Dadosfera - Analista de Dados

**Candidato:** Matheus Siqueira
**Data:** Janeiro/2026
**Reposit√≥rio:** MATHEUS_SIQUEIRA_DDF_TECH_012026
**Localiza√ß√£o:** Maring√° - PR

---

## üìã Item 0: Agilidade e Planejamento

Utilizei uma abordagem √Ågil (Kanban) para organizar as entregas deste case, priorizando a infraestrutura de dados (Bronze/Silver) antes da camada de intelig√™ncia e visualiza√ß√£o (Gold).

### üìÖ Status do Projeto

#### ‚úÖ Done (Conclu√≠do)
- [x] **Item 0:** Planejamento e Arquitetura
- [x] **Item 1:** Sele√ß√£o do Dataset (Brazilian E-Commerce Olist)
- [x] **Item 2:** Ingest√£o de Dados na Plataforma Dadosfera
- [x] **Item 3:** Cataloga√ß√£o e Dicion√°rio de Dados
- [x] **Item 4:** Valida√ß√£o de Qualidade de Dados (Great Expectations)
- [x] **Item 5:** Enriquecimento com IA (Feature Engineering / NLP)
- [x] **Item 6:** Modelagem Dimensional (Star Schema)
- [x] **Item 7:** Dashboard Anal√≠tico (Power BI)
- [x] **Item 8:** Orquestra√ß√£o de Pipelines (ETL)
- [x] **Item 9:** Data App Interativo (Streamlit)

---

## üíæ Item 1: Sobre a Base de Dados

Para simular um cen√°rio real de **E-commerce Brasileiro** com alta complexidade e volume (>100k registros), selecionei o **Brazilian E-Commerce Public Dataset by Olist**.

* **Motivo da Escolha:** O dataset oferece dados relacionais ricos (pedidos, clientes, produtos, geolocaliza√ß√£o) e dados desestruturados (reviews em texto), permitindo explorar todo o ciclo de vida dos dados exigido no case.
* **Volume:** A tabela principal `order_items` possui mais de 112.000 registros, atendendo ao requisito m√≠nimo do case.

---

## üîå Item 2 & 3: Integra√ß√£o e Explora√ß√£o (Dadosfera)

Realizei a ingest√£o dos arquivos CSV brutos para a camada de **Coleta** da Dadosfera. Os dados foram catalogados com descri√ß√µes funcionais e t√©cnicas para facilitar o self-service analytics por usu√°rios de neg√≥cio.

**Evid√™ncia da Carga e Cataloga√ß√£o na Plataforma:**
![Print da Dadosfera - Ingest√£o](assets/item23_coleta_dadosfera.png)

---

## üïµÔ∏è Item 4: Data Quality (Observabilidade)

Desenvolvi um pipeline de auditoria automatizada em Python que valida a integridade dos dados seguindo os princ√≠pios e regras do framework **Great Expectations**. Todo o motor de auditoria e monitoramento de sa√∫de da base est√° centralizado no arquivo **`data_quality.py`** na raiz do reposit√≥rio.

**Regras de Auditoria Aplicadas:**
1. **Consist√™ncia de Dom√≠nio:** Valida√ß√£o estat√≠stica para garantir que a coluna `review_score` contenha apenas valores entre 1 e 5 (Regra de Neg√≥cio).
2. **Integridade Referencial:** Verifica√ß√£o de nulidade na chave prim√°ria `review_id` para assegurar rastreabilidade √∫nica dos pedidos.
3. **Completo:** Gera√ß√£o de estat√≠sticas descritivas (M√≠nimo, M√°ximo e M√©dia) para monitoramento de sa√∫de da base utilizando conceitos avan√ßados de **Data Observability**.

**Evid√™ncia do Relat√≥rio de Qualidade:**
![Relat√≥rio de Data Quality](assets/item4_data_quality.png)

---

## ü§ñ Item 5: Enriquecimento de Dados com IA (Advanced NLP no Power BI)

O dataset original possu√≠a milhares de coment√°rios em texto livre (`review_comment_message`). Para estruturar esses dados, desenvolvi um pipeline de **Feature Engineering** com foco em An√°lise de Sentimento utilizando a biblioteca **spaCy** (modelo `pt_core_news_sm`).

**Solu√ß√£o Aplicada (Motor H√≠brido):**
Implementei um algoritmo de infer√™ncia que calibra a **Polaridade de Sentimento** correlacionando o texto com o *Ground Truth* (Nota do Cliente). Isso garante precis√£o sem√¢ntica para o idioma Portugu√™s (PT-BR), superando limita√ß√µes de modelos treinados apenas em ingl√™s.

**Integra√ß√£o via Power Query:**
A l√≥gica de IA otimizada para o ambiente do Power BI est√° detalhada no arquivo **`power_query_nlp.py`** na raiz do projeto. O c√≥digo foi integrado nativamente ao dashboard atrav√©s de um script Python executado no **Power Query (Python Step)**, permitindo o enriquecimento din√¢mico e automatizado do modelo de dados a cada atualiza√ß√£o.

* **Entrada (Input):** Texto bruto do cliente.
* **Processamento:** C√°lculo de polaridade matem√°tica via spaCy calibrada pelo score da avalia√ß√£o (70% Sem√¢ntica / 30% Nota).
* **Sa√≠da (Output):** M√©tricas de `Polaridade_IA` (-1.0 a +1.0) e Classifica√ß√£o categ√≥rica (`Positivo` üü¢ / `Neutro` üü° / `Negativo` üî¥) com alinhamento visual para logs.
* **Impacto:** Permitiu a cria√ß√£o de visuais avan√ßados no Dashboard baseados na intensidade do sentimento do cliente.

**Evid√™ncia da Integra√ß√£o no Power BI:**
![Script Python no Power Query](assets/powerquery_python_integration.png)

**Evid√™ncia do Pipeline de NLP:**
![Output do Script de IA](assets/item5_nlp.png)

---

## üìê Item 6: Modelagem de Dados

Desenvolvi uma modelagem **Star Schema (Fato/Dimens√£o)** no Power BI para garantir alta performance nas consultas DAX e facilidade de uso para o usu√°rio final. Adotei a nomenclatura padr√£o de Data Warehousing (`d` para dimens√µes, `f` para fatos).

### Estrutura do Modelo
* **Tabela Fato (`fOrderItems`):** Cont√©m os dados transacionais (granularidade por item vendido).
    * *M√©tricas:* Valor de Venda, Valor de Frete, Quantidade.
* **Dimens√µes (`d...`):** Tabelas auxiliares que fornecem contexto descritivo.
    * `dProducts` (Categorias e caracter√≠sticas dos itens).
    * `dOrders` (Status e datas do pedido).
    * `dCustomers` (Localiza√ß√£o e dados do cliente).
    * `dReviews` (Coment√°rios e notas de satisfa√ß√£o enriquecidas via IA).

### üîó Relacionamentos e Cardinalidade
As tabelas foram conectadas utilizando relacionamentos **Um-para-Muitos (1:*)** fluindo das dimens√µes para a fato, garantindo a filtragem correta (propaga√ß√£o de filtro):

1. **`dProducts` (1) ‚û°Ô∏è (*) `fOrderItems`**: Conectado via `product_id`. Analisa receita e volume por categoria de produto.
2. **`dOrders` (1) ‚û°Ô∏è (*) `fOrderItems`**: Conectado via `order_id`. Traz datas e status para cada item vendido.
3. **`dCustomers` (1) ‚û°Ô∏è (*) `dOrders`**: Conectado via `customer_id`. Segmenta pedidos e faturamento por Estado/Cidade do cliente.
4. **`dOrders` (1) ‚û°Ô∏è (*) `dReviews`**: Conectado via `order_id`. Correlaciona atrasos de entrega com a nota de satisfa√ß√£o.

**Diagrama de Entidade-Relacionamento (DER):**
![Modelagem Star Schema](assets/item6_modelagem.png)

---

## üìä Item 7 & B√¥nus 3: An√°lise de Dados (Power BI)

Optei por utilizar o **Power BI** para entregar uma an√°lise visual avan√ßada e interativa, conforme sugerido no **B√¥nus 3** do case.

**Link para o Arquivo:** [Dashboard Power BI (.pbix)](./dashboard_analise_olist.pbix)

**Visualiza√ß√µes Desenvolvidas:**
1. **KPIs Executivos:** Receita Total, Ticket M√©dio e Volumetria.
2. **An√°lise Geoespacial:** Mapa de calor de vendas por Estado (B√¥nus 2).
3. **S√©rie Temporal:** Evolu√ß√£o de vendas por m√™s/ano.
4. **An√°lise de Qualidade:** Distribui√ß√£o das notas de satisfa√ß√£o enriquecida com os dados de Reviews e NLP.

**Preview do Dashboard:**
![Dashboard Final Power BI](assets/item7_dashboard.png)

---

## üåä Item 8: Pipeline de Dados (Orquestra√ß√£o)

Para garantir a atualiza√ß√£o cont√≠nua e a governan√ßa dos dados, desenhei um pipeline de ingest√£o na Dadosfera que automatiza a coleta dos arquivos brutos (Raw Data) para a camada de processamento.

**Fluxo Desenhado:**
1. **Coleta:** Leitura incremental de arquivos CSV armazenados em Bucket S3 (`raw-data-olist`).
2. **Ingest√£o:** Carga para a Landing Zone da Dadosfera.
3. **Cataloga√ß√£o:** Registro autom√°tico de metadados t√©cnicos.
4. **Agendamento:** Execu√ß√£o di√°ria automatizada.

**Evid√™ncia do Pipeline Catalogado:**
![Pipeline Dadosfera](assets/item8_pipeline.png)

---

## üì± Item 9: Data App (Streamlit)

Desenvolvi uma aplica√ß√£o interativa utilizando o framework **Streamlit** (Python) para democratizar o acesso aos dados de satisfa√ß√£o. O app permite que gestores filtrem reviews por regi√£o e acompanhem KPIs financeiros e de log√≠stica em tempo real.

**Funcionalidades:**
* Filtros Din√¢micos de Regi√£o.
* Formata√ß√£o monet√°ria padr√£o BRL (R$).
* Comparativo de Metas (vs M√™s Anterior).
* Visualiza√ß√£o Dark Mode para alto contraste.

**Preview do App:**
![Data App Streamlit](assets/item9_data_app.png)

### üõ†Ô∏è Como Executar este Data App
O desenvolvimento foi realizado utilizando o **Google Colab**. Para reproduzir localmente:

1. **Pr√©-requisitos:** Python 3.9+, Streamlit, Pandas e Plotly.
2. **Instala√ß√£o:** `pip install streamlit pandas plotly`.
3. **Execu√ß√£o:** Navegue at√© a pasta do projeto e execute no terminal: `streamlit run app.py`.
4. **Acesso Remoto (Cloud):** Utilizado t√∫nel via **Ngrok** para deploy simulado durante o desenvolvimento.

---

## ‚è≠Ô∏è Pr√≥ximos Passos (Roadmap)
- Grava√ß√£o do v√≠deo de apresenta√ß√£o executiva (Item 10).
- Implementa√ß√£o de alertas autom√°ticos via Slack/Teams baseados na queda do NPS.
