# Case TÃ©cnico Dadosfera - Analista de Dados

**Candidato:** Matheus Siqueira  
**Data:** Janeiro/2026  
**RepositÃ³rio:** MATHEUS_SIQUEIRA_DDF_TECH_012026  

---

## ðŸ“‹ Item 0: Agilidade e Planejamento

Utilizei uma abordagem Ãgil (Kanban) para organizar as entregas deste case, priorizando a infraestrutura de dados (Bronze/Silver) antes da camada de inteligÃªncia e visualizaÃ§Ã£o (Gold).

### ðŸ“… Status do Projeto

#### âœ… Done (ConcluÃ­do)
- [x] **Item 0:** Planejamento e Arquitetura
- [x] **Item 1:** SeleÃ§Ã£o do Dataset (Brazilian E-Commerce Olist)
- [x] **Item 2:** IngestÃ£o de Dados na Plataforma Dadosfera
- [x] **Item 3:** CatalogaÃ§Ã£o e DicionÃ¡rio de Dados
- [x] **Item 4:** ValidaÃ§Ã£o de Qualidade de Dados (Great Expectations)
- [x] **Item 5:** Enriquecimento com IA (Feature Engineering / NLP)
- [x] **Item 6:** Modelagem Dimensional (Star Schema)
- [x] **Item 7:** Dashboard AnalÃ­tico (Power BI)
- [x] **Item 8:** OrquestraÃ§Ã£o de Pipelines (ETL)
- [x] **Item 9:** Data App Interativo (Streamlit)
- [x] **Item 10:** GravaÃ§Ã£o do VÃ­deo de ApresentaÃ§Ã£o (Storytelling)

---

## ðŸ’¾ Item 1: Sobre a Base de Dados

Para simular um cenÃ¡rio real de **E-commerce Brasileiro** com alta complexidade e volume (>100k registros), selecionei o **Brazilian E-Commerce Public Dataset by Olist**.

* **Motivo da Escolha:** O dataset oferece dados relacionais ricos (pedidos, clientes, produtos, geolocalizaÃ§Ã£o) e dados desestruturados (reviews em texto), permitindo explorar todo o ciclo de vida dos dados exigido no case.
* **Volume:** A tabela principal `order_items` possui mais de 112.000 registros, atendendo ao requisito mÃ­nimo do case.

---

## ðŸ”Œ Item 2 & 3: IntegraÃ§Ã£o e ExploraÃ§Ã£o (Dadosfera)

Realizei a ingestÃ£o dos arquivos CSV brutos para a camada de **Coleta** da Dadosfera. Os dados foram catalogados com descriÃ§Ãµes funcionais e tÃ©cnicas para facilitar o self-service analytics por usuÃ¡rios de negÃ³cio.

> ðŸ“˜ **DocumentaÃ§Ã£o TÃ©cnica:** Para detalhes aprofundados sobre a linhagem, tipagem e regras de negÃ³cio aplicadas em cada tabela (Silver/Gold), consulte o **[DicionÃ¡rio de Dados TÃ©cnico](./DATA_DICTIONARY.md)**.

**EvidÃªncia da Carga e CatalogaÃ§Ã£o na Plataforma:**

![Print da Dadosfera - IngestÃ£o](assets/item23_coleta_dadosfera.png)

---

## ðŸ•µï¸ Item 4: Data Quality (Observabilidade)

Implementei um pipeline de auditoria automatizada fundamentado em **Data Contracts** e observabilidade de dados. Utilizei uma lÃ³gica de validaÃ§Ã£o inspirada no framework *Great Expectations* para garantir que apenas dados Ã­ntegros e confiÃ¡veis avancem para a camada de modelagem. Todo o motor de auditoria e monitoramento estÃ¡ centralizado no arquivo **`data_quality.py`** na raiz do repositÃ³rio.

**Regras de Auditoria Aplicadas:**
* **ConsistÃªncia de DomÃ­nio:** ValidaÃ§Ã£o estatÃ­stica rigorosa para garantir que a coluna `review_score` esteja dentro do intervalo esperado de **1 a 5**.
* **Integridade Referencial:** Check de completude na **Chave PrimÃ¡ria** `review_id` (Zero Nulls), assegurando a unicidade e rastreabilidade total dos registros.
* **Health Check & Monitoring:** GeraÃ§Ã£o automÃ¡tica de mÃ©tricas descritivas (MÃ­nimo, MÃ¡ximo e MÃ©dia) para monitoramento de saÃºde da base e detecÃ§Ã£o precoce de anomalias.

**EvidÃªncia do RelatÃ³rio de Qualidade:**

![RelatÃ³rio de Data Quality](assets/item4_data_quality.png)

---

## ðŸ¤– Item 5: Enriquecimento com IA (Advanced NLP no Power Query)

Para processar o volume de textos desestruturados (`review_comment_message`), desenvolvi um motor de **NLP** robusto utilizando a biblioteca **spaCy** (modelo `pt_core_news_sm`).

**Diferencial TÃ©cnico: Motor de InferÃªncia HÃ­brida**
Implementei uma **CalibraÃ§Ã£o de Ground Truth**, onde o algoritmo correlaciona a semÃ¢ntica extraÃ­da via IA com a nota real deixada pelo cliente, calibrando a polaridade final para refletir a experiÃªncia real do usuÃ¡rio.

**IntegraÃ§Ã£o e Portabilidade:**
A lÃ³gica estÃ¡ encapsulada no script **`power_query_nlp.py`**. O cÃ³digo foi portado para o ambiente do **Power Query (Python Step)**, permitindo o enriquecimento dinÃ¢mico do modelo de dados diretamente no Power BI a cada refresh.

* **OtimizaÃ§Ã£o Upstream:** Implementei uma filtragem prÃ©via no Power Query para enviar ao script Python apenas as colunas estritamente necessÃ¡rias (`id`, `score`, `text`), reduzindo o tempo de processamento e serializaÃ§Ã£o de dados.
* **MÃ©tricas de SaÃ­da:** GeraÃ§Ã£o das colunas `Polaridade_IA` (-1.0 a +1.0) e `Sentimento_IA` (Positivo ðŸŸ¢ / Neutro ðŸŸ¡ / Negativo ðŸ”´).

**EvidÃªncia da IntegraÃ§Ã£o no Power BI:**

![Script Python no Power Query](assets/powerquery_python_integration.png)

**EvidÃªncia do Pipeline de NLP:**

![Output do Script de IA](assets/item5_nlp.png)

---

## ðŸ“ Item 6: Modelagem de Dados

Desenvolvi uma modelagem **Star Schema (Fato/DimensÃ£o)** no Power BI para garantir alta performance nas consultas DAX e facilidade de uso para o usuÃ¡rio final.

### ðŸ—ï¸ Engenharia de Dados e Performance (Silver Layer)
Apliquei conceitos avanÃ§ados de engenharia na etapa de transformaÃ§Ã£o (Power Query) para garantir escalabilidade e governanÃ§a:

1.  **GovernanÃ§a (Naming Conventions):** Adotei estritamente o padrÃ£o **`snake_case`** (ex: `product_category_name` em vez de `Nome da Categoria`) e removi acentos/caracteres especiais.
    * *Motivo:* Garantir interoperabilidade imediata caso o modelo seja migrado para Data Lakes (Parquet/Delta) ou Bancos SQL, onde espaÃ§os e acentos costumam quebrar pipelines.
2.  **Vertical Partitioning (Performance):** Realizei a remoÃ§Ã£o agressiva de colunas de alta cardinalidade nÃ£o utilizadas (ex: `product_description`) antes da carga.
    * *Impacto:* ReduÃ§Ã£o drÃ¡stica do consumo de memÃ³ria do motor VertiPaq e aceleraÃ§Ã£o do refresh.
3.  **Type Safety & Localization:** ImplementaÃ§Ã£o de tratamento explÃ­cito de locale (`en-US`) na camada M.
    * *Motivo:* Garantir que preÃ§os e coordenadas geogrÃ¡ficas vindos de CSVs internacionais (separador decimal ponto) sejam interpretados corretamente, evitando erros de magnitude financeira.
4.  **Data Enrichment (BÃ´nus 2):** Enriquecimento da dimensÃ£o de clientes (`dCustomers`) com coordenadas exatas de **Latitude e Longitude**.
    * *TÃ©cnica:* Realizei o agrupamento (Group By) da base de geolocalizaÃ§Ã£o (reduzindo 1MM+ linhas para chaves Ãºnicas de CEP) antes de realizar o *Merge*, garantindo performance sem perder precisÃ£o geogrÃ¡fica.

### Estrutura do Modelo
* **Tabela Fato (`fOrderItems`):** ContÃ©m os dados transacionais (granularidade por item vendido).
    * *MÃ©tricas:* Valor de Venda, Valor de Frete, Quantidade.
* **DimensÃµes (`d...`):** Tabelas auxiliares que fornecem contexto descritivo.
    * `dProducts` (Categorias higienizadas e padronizadas).
    * `dOrders` (Status e datas do ciclo de vida do pedido).
    * `dCustomers` (LocalizaÃ§Ã£o geogrÃ¡fica por Estado/Cidade).
    * `dReviews` (ComentÃ¡rios e notas de satisfaÃ§Ã£o enriquecidas via IA).

**Diagrama de Entidade-Relacionamento (DER):**

![Modelagem Star Schema](assets/item6_modelagem.png)

---

## ðŸ“Š Item 7 & BÃ´nus 3: AnÃ¡lise de Dados (Power BI & SQL)

Para cumprir o requisito de anÃ¡lise exploratÃ³ria e validaÃ§Ã£o de categorias, utilizei o **SQL Lab** da Dadosfera (Engine Snowflake) antes de partir para a visualizaÃ§Ã£o no Power BI.

### ðŸ” ValidaÃ§Ã£o ExploratÃ³ria (SQL)
**Objetivo:** Validar a distribuiÃ§Ã£o de produtos por categoria diretamente na fonte (Silver Layer), assegurando a integridade dos dados antes da modelagem.

**EvidÃªncia da ExecuÃ§Ã£o (Query + Resultado):**

![Resultado SQL](assets/item7_sql_query.png)

### ðŸš€ Dashboard Executivo (Power BI)
Desenvolvi um **Data App** no Power BI dividido em duas camadas estratÃ©gicas, unindo Engenharia de Dados robusta com UX avanÃ§ado via HTML/SVG (DAX).

**Link para o Arquivo:** [Dashboard Power BI (.pbix)](./dashboard_analise_olist.pbix)

#### 1. Executive Insights (VisÃ£o Macro/EstratÃ©gica)
Focada no C-Level, consolidando a saÃºde financeira e logÃ­stica.
* **Header DinÃ¢mico:** VisualizaÃ§Ã£o *Glassmorphism* com KPIs de Faturamento e Sentimento Geral.
* **Breakdown de LogÃ­stica:** AnÃ¡lise de gargalos (Lead Time) separando AprovaÃ§Ã£o, SeparaÃ§Ã£o e Last Mile.
* **Top 3 Categorias:** Ranking inteligente que cruza Receita com PercepÃ§Ã£o do Cliente (IA Score).

![Executive Insights Dashboard](assets/item10_powerbi1.png)

#### 2. Operational Intelligence (VisÃ£o Micro/TÃ¡tica)
Focada em identificar ofensores, produtos crÃ­ticos e oportunidades geogrÃ¡ficas.
* **Operational Header (Ranking em Tempo Real):** IdentificaÃ§Ã£o automÃ¡tica do "Best Seller", "Top RegiÃ£o" e "Ãrea de AtenÃ§Ã£o CrÃ­tica" (pior sentimento).
* **Product Deep Dive (Card 360Âº):** DiagnÃ³stico automÃ¡tico que cruza Vendas vs. Sentimento para classificar produtos (ex: "Risco de Churn" ou "Estrela de Vendas").
* **Geo-Intelligence:** Mapa de calor utilizando coordenadas exatas (Lat/Long) para identificar densidade de demanda.

![Operational Intelligence Dashboard](assets/item10_powerbi2.png)

#### Destaques TÃ©cnicos
* **UX/UI AvanÃ§ado:** SubstituiÃ§Ã£o de cartÃµes nativos por componentes HTML/CSS injetados via DAX para flexibilidade total de design.
* **OtimizaÃ§Ã£o da DimensÃ£o Tempo (`dTime`):** Tabela dimensÃ£o otimizada com granularidade de minutos para reduzir cardinalidade e melhorar performance do VertiPaq.
* **GlossÃ¡rio Integrado:** ImplementaÃ§Ã£o de Tooltips explicativas (Mini-manual) para garantir a governanÃ§a e entendimento das mÃ©tricas de IA pelo usuÃ¡rio final.

---

## ðŸŒŠ Item 8: Pipeline de Dados (OrquestraÃ§Ã£o)

Para garantir a atualizaÃ§Ã£o contÃ­nua e a governanÃ§a dos dados, desenhei um pipeline de ingestÃ£o na Dadosfera que automatiza a coleta dos arquivos brutos (Raw Data).

**Fluxo Desenhado:**
1. **Coleta:** Leitura incremental de arquivos CSV em Bucket S3.
2. **IngestÃ£o:** Carga para a Landing Zone da Dadosfera.
3. **CatalogaÃ§Ã£o:** Registro automÃ¡tico de metadados tÃ©cnicos.
4. **Agendamento:** ExecuÃ§Ã£o diÃ¡ria automatizada.

**EvidÃªncia do Pipeline Catalogado:**

![Pipeline Dadosfera](assets/item8_pipeline.png)

---

## ðŸ“± Item 9: Data App (Streamlit)

Desenvolvi uma aplicaÃ§Ã£o interativa utilizando o framework **Streamlit** (Python) para democratizar o acesso aos dados de satisfaÃ§Ã£o. O app permite que gestores filtrem reviews por regiÃ£o e acompanhem KPIs em tempo real.

**Funcionalidades:**
* Filtros DinÃ¢micos de RegiÃ£o.
* FormataÃ§Ã£o monetÃ¡ria padrÃ£o BRL (R$).
* Comparativo de Metas (vs MÃªs Anterior).
* VisualizaÃ§Ã£o Dark Mode para alto contraste.

**Preview do App:**

![Data App Streamlit](assets/item9_data_app.png)

### ðŸ› ï¸ Como Executar este Data App
O desenvolvimento foi realizado utilizando o **Google Colab**. Para reproduzir localmente:

1. **PrÃ©-requisitos:** Python 3.9+, Streamlit, Pandas e Plotly.
2. **InstalaÃ§Ã£o:** `pip install streamlit pandas plotly`.
3. **ExecuÃ§Ã£o:** Navegue atÃ© a pasta do projeto e execute no terminal: `streamlit run app.py`.
4. **Acesso Remoto (Cloud):** Utilizado tÃºnel via **Ngrok** para deploy simulado durante o desenvolvimento.

---

## ðŸ“¹ Item 10: ApresentaÃ§Ã£o Executiva (VÃ­deo)

Confira a apresentaÃ§Ã£o completa da soluÃ§Ã£o, onde detalho a arquitetura tÃ©cnica, as decisÃµes de engenharia e navego pelas funcionalidades de toda a soluÃ§Ã£o.

[![Assista ao VÃ­deo](https://img.youtube.com/vi/Y4eDg1Gc/0.jpg)](https://www.youtube.com/watch?v=nt_Y4eDg1Gc)

> **Nota:** Clique na imagem acima para ser redirecionado ao player do YouTube.
