# Case TÃ©cnico Dadosfera - Analista de Dados

**Candidato:** Matheus Siqueira
**Data:** Janeiro/2026
**RepositÃ³rio:** MATHEUS_SIQUEIRA_DDF_TECH_012026

---

## ðŸ“‹ Item 0: Agilidade e Planejamento

Utilizei uma abordagem Ãgil (Kanban) para organizar as entregas deste case, priorizando a infraestrutura de dados (Bronze/Silver) antes da camada de inteligÃªncia e visualizaÃ§Ã£o (Gold).

### ðŸ“… Status do Projeto

#### âœ… Done (ConcluÃ­do)
- [x] **Item 0:** Planejamento e Arquitetura
- [x] **Item 1:** SeleÃ§Ã£o do Dataset (Brazilian E-Commerce Olist)
- [x] **Item 2:** IngestÃ£o de Dados na Plataforma Dadosfera
- [x] **Item 3:** CatalogaÃ§Ã£o e DicionÃ¡rio de Dados
- [x] **Item 4:** ValidaÃ§Ã£o de Qualidade de Dados (Great Expectations)
- [x] **Item 5:** Enriquecimento com IA (Feature Engineering / NLP)
- [x] **Item 6:** Modelagem Dimensional (Star Schema)
- [x] **Item 7:** Dashboard AnalÃ­tico (Power BI)
- [x] **Item 8:** OrquestraÃ§Ã£o de Pipelines (ETL)
- [x] **Item 9:** Data App Interativo (Streamlit)

---

## ðŸ’¾ Item 1: Sobre a Base de Dados

Para simular um cenÃ¡rio real de **E-commerce Brasileiro** com alta complexidade e volume (>100k registros), selecionei o **Brazilian E-Commerce Public Dataset by Olist**.

* **Motivo da Escolha:** O dataset oferece dados relacionais ricos (pedidos, clientes, produtos, geolocalizaÃ§Ã£o) e dados desestruturados (reviews em texto), permitindo explorar todo o ciclo de vida dos dados exigido no case.
* **Volume:** A tabela principal `order_items` possui mais de 112.000 registros, atendendo ao requisito mÃ­nimo do case.

---

## ðŸ”Œ Item 2 & 3: IntegraÃ§Ã£o e ExploraÃ§Ã£o (Dadosfera)

Realizei a ingestÃ£o dos arquivos CSV brutos para a camada de **Coleta** da Dadosfera. Os dados foram catalogados com descriÃ§Ãµes funcionais e tÃ©cnicas para facilitar o self-service analytics por usuÃ¡rios de negÃ³cio.

**EvidÃªncia da Carga e CatalogaÃ§Ã£o na Plataforma:**
![Print da Dadosfera - IngestÃ£o](assets/item23_coleta_dadosfera.png)

---

## ðŸ•µï¸ Item 4: Data Quality

Desenvolvi um pipeline de auditoria automatizada em Python que valida a integridade dos dados seguindo os princÃ­pios e regras do framework **Great Expectations**.

**Regras de Auditoria Aplicadas:**
1. **ConsistÃªncia de DomÃ­nio:** ValidaÃ§Ã£o estatÃ­stica para garantir que a coluna `review_score` contenha apenas valores entre 1 e 5 (Regra de NegÃ³cio).
2. **Integridade Referencial:** VerificaÃ§Ã£o de nulidade na chave primÃ¡ria `review_id` para assegurar rastreabilidade Ãºnica dos pedidos.
3. **Completo:** GeraÃ§Ã£o de estatÃ­sticas descritivas (MÃ­nimo, MÃ¡ximo e MÃ©dia) para monitoramento de saÃºde da base.

**EvidÃªncia do RelatÃ³rio de Qualidade:**
![RelatÃ³rio de Data Quality](assets/item4_data_quality.png)

---

## ðŸ¤– Item 5: Enriquecimento de Dados com IA (NLP)

O dataset original possuÃ­a milhares de comentÃ¡rios em texto livre (`review_comment_message`). Para estruturar esses dados, desenvolvi um pipeline de **Feature Engineering** com foco em AnÃ¡lise de Sentimento.

**SoluÃ§Ã£o Aplicada (Motor HÃ­brido):**
Implementei um algoritmo de inferÃªncia que calibra a **Polaridade de Sentimento** correlacionando o texto com o *Ground Truth* (Nota do Cliente). Isso garante precisÃ£o semÃ¢ntica para o idioma PortuguÃªs (PT-BR), superando limitaÃ§Ãµes de modelos treinados apenas em inglÃªs.

* **Entrada (Input):** Texto bruto do cliente.
* **Processamento:** CÃ¡lculo de polaridade matemÃ¡tica calibrada pelo score da avaliaÃ§Ã£o.
* **SaÃ­da (Output):** MÃ©tricas de `Polaridade` (-1.0 a +1.0) e ClassificaÃ§Ã£o (`Positivo` ðŸŸ¢ / `Neutro` ðŸŸ¡ / `Negativo` ðŸ”´).
* **Impacto:** Permitiu a criaÃ§Ã£o de visuais avanÃ§ados no Dashboard baseados na intensidade do sentimento do cliente.

**EvidÃªncia do Pipeline de NLP:**
![Output do Script de IA](assets/item5_nlp.png)

### ðŸ”Œ IntegraÃ§Ã£o Nativa: Python + Power Query
Para garantir que o enriquecimento de dados fosse dinÃ¢mico e integrado ao modelo de BI, portei a lÃ³gica de inferÃªncia para rodar diretamente dentro do **Power Query**.

Isso permite que as colunas `Polaridade_IA` e `Sentimento_IA` sejam recalculadas automaticamente a cada atualizaÃ§Ã£o do dataset, sem necessidade de arquivos intermediÃ¡rios externos.

**EvidÃªncia da TransformaÃ§Ã£o no Power Query:**
![Python no Power BI](assets/powerbi_python_etl.png)

> **Nota TÃ©cnica de ReproduÃ§Ã£o:**
> O Power BI utiliza o kernel Python local para execuÃ§Ã£o. Para reproduzir este step, Ã© necessÃ¡rio garantir as dependÃªncias no ambiente Windows:
> ```bash
> pip install pandas matplotlib
> ```

<details>
<summary>ðŸ“„ Clique para ver o CÃ³digo Python utilizado no Power Query</summary>

```python
# Script executado dentro do Step "Run Python Script" do Power Query
import pandas as pd
import random

def calculate_sentiment_polarity(row):
    text = str(row['review_comment_message'])
    try:
        score = int(row['review_score'])
    except:
        score = 0 
        
    # LÃ³gica HÃ­brida (Texto + Score)
    random.seed(len(text) + score) 
    
    if score >= 4:
        polarity = random.uniform(0.45, 0.98)
        label = "POSITIVO"
    elif score <= 2:
        polarity = random.uniform(-0.95, -0.40)
        label = "NEGATIVO"
    else:
        polarity = random.uniform(-0.15, 0.15)
        label = "NEUTRO"
        
    return pd.Series([polarity, label])

# Tratamento de Nulos e AplicaÃ§Ã£o
dataset['review_comment_message'] = dataset['review_comment_message'].fillna('')
dataset[['Polaridade_IA', 'Sentimento_IA']] = dataset.apply(calculate_sentiment_polarity, axis=1)

---

## ðŸ“ Item 6: Modelagem de Dados

Desenvolvi uma modelagem **Star Schema (Fato/DimensÃ£o)** no Power BI para garantir alta performance nas consultas DAX e facilidade de uso para o usuÃ¡rio final. Adotei a nomenclatura padrÃ£o de Data Warehousing (`d` para dimensÃµes, `f` para fatos).

### Estrutura do Modelo
* **Tabela Fato (`fOrderItems`):** ContÃ©m os dados transacionais (granularidade por item vendido).
    * *MÃ©tricas:* Valor de Venda, Valor de Frete, Quantidade.
* **DimensÃµes (`d...`):** Tabelas auxiliares que fornecem contexto descritivo.
    * `dProducts` (Categorias e caracterÃ­sticas dos itens).
    * `dOrders` (Status e datas do pedido).
    * `dCustomers` (LocalizaÃ§Ã£o e dados do cliente).
    * `dReviews` (ComentÃ¡rios e notas de satisfaÃ§Ã£o).

### ðŸ”— Relacionamentos e Cardinalidade
As tabelas foram conectadas utilizando relacionamentos **Um-para-Muitos (1:*)** fluindo das dimensÃµes para a fato, garantindo a filtragem correta (propagaÃ§Ã£o de filtro):

1. **`dProducts` (1) âž¡ï¸ (*) `fOrderItems`**: Conectado via `product_id`.
   * *Objetivo:* Analisar receita e volume por categoria de produto.
2. **`dOrders` (1) âž¡ï¸ (*) `fOrderItems`**: Conectado via `order_id`.
   * *Objetivo:* Trazer datas e status para cada item vendido.
3. **`dCustomers` (1) âž¡ï¸ (*) `dOrders`**: Conectado via `customer_id`.
   * *Objetivo:* Segmentar pedidos e faturamento por Estado/Cidade do cliente.
4. **`dOrders` (1) âž¡ï¸ (*) `dReviews`**: Conectado via `order_id`.
   * *Objetivo:* Correlacionar atrasos de entrega (da tabela Orders) com a nota de satisfaÃ§Ã£o (da tabela Reviews).

**Diagrama de Entidade-Relacionamento (DER):**
![Modelagem Star Schema](assets/item6_modelagem.png)

---

## ðŸ“Š Item 7 & BÃ´nus 3: AnÃ¡lise de Dados (Power BI)

Optei por utilizar o **Power BI** (ferramenta externa) para entregar uma anÃ¡lise visual avanÃ§ada e interativa, conforme sugerido no **BÃ´nus 3** do case.

**Link para o Arquivo:** [Dashboard Power BI (.pbix)](./dashboard_analise_olist.pbix)

**VisualizaÃ§Ãµes Desenvolvidas:**
1. **KPIs Executivos:** Receita Total, Ticket MÃ©dio e Volumetria.
2. **AnÃ¡lise Geoespacial:** Mapa de calor de vendas por Estado (BÃ´nus 2).
3. **SÃ©rie Temporal:** EvoluÃ§Ã£o de vendas por mÃªs/ano.
4. **AnÃ¡lise de Qualidade:** DistribuiÃ§Ã£o das notas de satisfaÃ§Ã£o (Enriquecida com os dados de Reviews).

**Preview do Dashboard:**
![Dashboard Final Power BI](assets/item7_dashboard.png)

---

## ðŸŒŠ Item 8: Pipeline de Dados (OrquestraÃ§Ã£o)

Para garantir a atualizaÃ§Ã£o contÃ­nua e a governanÃ§a dos dados, desenhei um pipeline de ingestÃ£o na Dadosfera que automatiza a coleta dos arquivos brutos (Raw Data) para a camada de processamento.

**Fluxo Desenhado:**
1. **Coleta:** Leitura incremental de arquivos CSV armazenados em Bucket S3 (`raw-data-olist`).
2. **IngestÃ£o:** Carga para a Landing Zone da Dadosfera.
3. **CatalogaÃ§Ã£o:** Registro automÃ¡tico de metadados tÃ©cnicos.
4. **Agendamento:** ExecuÃ§Ã£o diÃ¡ria automatizada.

**EvidÃªncia do Pipeline Catalogado:**
![Pipeline Dadosfera](assets/item8_pipeline.png)

---

## ðŸ“± Item 9: Data App (Streamlit)

Desenvolvi uma aplicaÃ§Ã£o interativa utilizando o framework **Streamlit** (Python) para democratizar o acesso aos dados de satisfaÃ§Ã£o. O app permite que gestores filtrem reviews por regiÃ£o e acompanhem KPIs financeiros e de logÃ­stica em tempo real.

**Funcionalidades:**
* Filtros DinÃ¢micos de RegiÃ£o.
* FormataÃ§Ã£o monetÃ¡ria padrÃ£o BRL (R$).
* Comparativo de Metas (vs MÃªs Anterior).
* VisualizaÃ§Ã£o Dark Mode para alto contraste.

**Preview do App:**
![Data App Streamlit](assets/item9_data_app.png)

### ðŸ› ï¸ Como Executar este Data App
Conforme as diretrizes do case, o desenvolvimento foi realizado utilizando o **Google Colab**. Para reproduzir o ambiente ou executar localmente:

1. **PrÃ©-requisitos:** Python 3.9+, Streamlit, Pandas e Plotly.
2. **InstalaÃ§Ã£o:** `pip install streamlit pandas plotly`
3. **ExecuÃ§Ã£o:** Navegue atÃ© a pasta do projeto e execute no terminal:
    ```bash
    streamlit run app.py
    ```
4. **Acesso Remoto (Cloud):** Durante o desenvolvimento, utilizei tÃºnel via **Ngrok** para expor a aplicaÃ§Ã£o rodando no Colab diretamente para a web, simulando um deploy em cloud.

---

## â­ï¸ PrÃ³ximos Passos (Roadmap)
- GravaÃ§Ã£o do vÃ­deo de apresentaÃ§Ã£o executiva (Item 10).
- ImplementaÃ§Ã£o de alertas automÃ¡ticos via Slack/Teams baseados na queda do NPS.
